{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entire code works perfectly fine!!!\n",
    "\n",
    "# At NLP Course L3-S18 (8:30), I did nltk.download('punkt') to download just 'punkt', \n",
    "# instead of what he shows using nltk.download() and that GUI thing - and it worked !\n",
    "\n",
    "# I increased the disk space on Ubuntu_ANN to 60 GB !!!\n",
    "# See Windows machine bookmarks -> 'Ubuntu_ANN_stuff' folder for steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is for my NLP Udemy class, which can be found at:\n",
    "# https://deeplearningcourses.com/c/data-science-natural-language-processing-in-python\n",
    "# https://www.udemy.com/data-science-natural-language-processing-in-python\n",
    "# It is written in such a way that tells a story.\n",
    "# i.e. So you can follow a thought process of starting from a\n",
    "# simple idea, hitting an obstacle, overcoming it, etc.\n",
    "# i.e. It is not optimized for anything.\n",
    "\n",
    "# Author: http://lazyprogrammer.me\n",
    "# from __future__ import print_function, division\n",
    "# from future.utils import iteritems\n",
    "# from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()     # for stem words\n",
    "\n",
    "# from http://www.lextek.com/manuals/onix/stopwords1.html\n",
    "stopwords = set(w.rstrip() for w in open('stopwords.txt')) # this stopwords.txt file is there\n",
    "                                                           # in the current folder !\n",
    "# note: an alternative source of stopwords\n",
    "# from nltk.corpus import stopwords\n",
    "# stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajvarghese\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\rajvarghese\\AppData\\Local\\Continuum\\anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# load the reviews\n",
    "# data courtesy of http://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html\n",
    "\n",
    "# BeautifulSoup is for reading XML data - need to look at ATBSWP book!\n",
    "\n",
    "positive_reviews = BeautifulSoup(open('electronics/positive.review').read()) # think like LIST\n",
    "positive_reviews = positive_reviews.findAll('review_text')\n",
    "\n",
    "negative_reviews = BeautifulSoup(open('electronics/negative.review').read())\n",
    "negative_reviews = negative_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.ResultSet"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(positive_reviews)\n",
    "\n",
    "# think of 'positive_reviews' as a LIST, with delimiters being <review_text>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positive_reviews) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<review_text>\n",
       "I purchased this unit due to frequent blackouts in my area and 2 power supplies going bad.  It will run my cable modem, router, PC, and LCD monitor for 5 minutes.  This is more than enough time to save work and shut down.   Equally important, I know that my electronics are receiving clean power.\n",
       "\n",
       "I feel that this investment is minor compared to the loss of valuable data or the failure of equipment due to a power spike or an irregular power supply.\n",
       "\n",
       "As always, Amazon had it to me in &lt;2 business days\n",
       "</review_text>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nI purchased this unit due to frequent blackouts in my area and 2 power supplies going bad.  It will run my cable modem, router, PC, and LCD monitor for 5 minutes.  This is more than enough time to save work and shut down.   Equally important, I know that my electronics are receiving clean power.\\n\\nI feel that this investment is minor compared to the loss of valuable data or the failure of equipment due to a power spike or an irregular power supply.\\n\\nAs always, Amazon had it to me in <2 business days\\n'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_reviews[0].text      \n",
    "\n",
    "# note the .text attribute of each review! This .text will work only on individual reviews and not\n",
    "# on the whole 'positive_review' object (which I think of as a List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(positive_reviews))\n",
    "print(len(negative_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are more positive reviews than negative reviews         # wrong I think !\n",
    "# so let's take a random sample so we have balanced classes\n",
    "# np.random.shuffle(positive_reviews)\n",
    "# positive_reviews = positive_reviews[:len(negative_reviews)]\n",
    "\n",
    "# we can also oversample the negative reviews\n",
    "diff = len(positive_reviews) - len(negative_reviews)\n",
    "idxs = np.random.choice(len(negative_reviews), size=diff)\n",
    "\n",
    "extra = [negative_reviews[i] for i in idxs]\n",
    "negative_reviews += extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff  # heck, just like I thought!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan of Action:\n",
    "# STEP 1\n",
    "- First TOKENIZE each review (so each review is now a LIST of words).   \n",
    "- Then do STEMMING and \n",
    "- Finally do STOP WORD removal\n",
    "\n",
    "All this is done in a single function called my_tokenizer() --- see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's just try to tokenize the text using nltk's tokenizer\n",
    "# let's take the first review for example:\n",
    "# t = positive_reviews[0]\n",
    "# nltk.tokenize.word_tokenize(t.text)\n",
    "#\n",
    "# notice how it doesn't downcase, so It != it\n",
    "# not only that, but do we really want to include the word \"it\" anyway?\n",
    "# you can imagine it wouldn't be any more common in a positive review than a negative review\n",
    "# so it might only add noise to our model.\n",
    "# so let's create a function that does all this pre-processing for us\n",
    "\n",
    "def my_tokenizer(s):    # s is going to be each review - see later\n",
    "    s = s.lower() # downcase\n",
    "    tokens = nltk.tokenize.word_tokenize(s) # split string into words (tokens)\n",
    "    tokens = [t for t in tokens if len(t) > 2] # remove short words, they're probably not useful\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # put words into base form ie. stemming\n",
    "    tokens = [t for t in tokens if t not in stopwords] # remove stopwords\n",
    "    return tokens\n",
    "\n",
    "# this function is run on each review ie. each row of input data! - see below\n",
    "# understood!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['purchased', 'this', 'unit', 'due', 'frequent', 'blackout', 'power', 'supply', 'bad', 'run', 'cable', 'modem', 'router', 'lcd', 'monitor', 'minute', 'this', 'time', 'save', 'shut', 'equally', 'electronics', 'receiving', 'clean', 'power', 'feel', 'this', 'investment', 'minor', 'compared', 'loss', 'valuable', 'data', 'failure', 'equipment', 'due', 'power', 'spike', 'irregular', 'power', 'supply', 'amazon', 'business', 'day']\n"
     ]
    }
   ],
   "source": [
    "# Just looking at what the above function does !!! It splits each review into a final LIST of words \n",
    "# after stemming and stopword removal\n",
    "print(my_tokenizer(positive_reviews[0].text ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Had to do some NLTK Downloader stuff - eg to download package 'punkt', etc. - see vid\n",
    "# This is a one time deal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2\n",
    "### Creating a Sparse Matrix (where each word is represented by the no. of times it occurs in the review divided by the total number of words in the review x 100):\n",
    "- first create a DICTIONARY called \"word_index_map\" \n",
    "- then use the Dictionary on each review to flip zeros to 1's, count the number of times each word appears in the review and then finally divide by the total number of words in the review - this will normalize the values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First step: Here we are just creating a DICTIONARY called 'word_index_map'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(word_index_map): 11081\n"
     ]
    }
   ],
   "source": [
    "# create a word-to-index map so that we can create our word-frequency vectors later\n",
    "# let's also save the tokenized versions so we don't have to tokenize again later\n",
    "\n",
    "word_index_map = {}   # this is going to be our dictionary of words\n",
    "current_index = 0\n",
    "positive_tokenized = []\n",
    "negative_tokenized = []\n",
    "orig_reviews = []\n",
    "\n",
    "for review in positive_reviews:\n",
    "    orig_reviews.append(review.text)\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    positive_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "for review in negative_reviews:\n",
    "    orig_reviews.append(review.text)\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "print(\"len(word_index_map):\", len(word_index_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Second Step: apply the Dictionary on each review:\n",
    "- First create an array of 0's for each review\n",
    "- Apply the dictionary to each review to flip the 0's to 1's, count the number of times each word appears in the review and then finally divide by the total number of words in the review - this will normalize the values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will first count the number of times each word appears in a review using the \n",
    "# above Dictionary, \n",
    "# then divide by the total number of words in the review. So this will normalize the values.\n",
    "# ie. we will get probability values for each word - which are between 0 and 1 I think !\n",
    "# This is good for Logistic Regression and Deep Learning models - think of the sigmoid curve.\n",
    "# Plus we dont want to be modelling for the length of the reviews - see the video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_vector(tokens, label):\n",
    "    x = np.zeros(len(word_index_map) + 1) # last element is for the label\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] += 1   # this will count the number of times each word occurs in a review\n",
    "    x = x / x.sum() # normalize it before setting label - wow!\n",
    "    x[-1] = label\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a NxD Matrix of zeros\n",
    "N = len(positive_tokenized) + len(negative_tokenized)\n",
    "# (N x D+1 matrix - keeping them together for now so we can shuffle more easily later\n",
    "data = np.zeros((N, len(word_index_map) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for tokens in positive_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 1)     # so for these the last col will be the label = 1\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "\n",
    "for tokens in negative_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 0)    # for these the last col will be the label = 0\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "\n",
    "# shuffle the data and create train/test splits\n",
    "# try it multiple times!\n",
    "orig_reviews, data = shuffle(orig_reviews, data)\n",
    "\n",
    "X = data[:,:-1]\n",
    "Y = data[:,-1]\n",
    "\n",
    "# last 100 rows will be test\n",
    "Xtrain = X[:-100,]\n",
    "Ytrain = Y[:-100,]\n",
    "Xtest = X[-100:,]\n",
    "Ytest = Y[-100:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.04444444, 0.04444444, ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:1, :]   # Just looking at the first row of the final Sparse Matrix !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7736842105263158\n",
      "Test accuracy: 0.83\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"Train accuracy:\", model.score(Xtrain, Ytrain))\n",
    "print(\"Test accuracy:\", model.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's look at the weights for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit -0.6854018461910513\n",
      "bad -0.7948788331104824\n",
      "cable 0.7205417929305832\n",
      "time -0.7055375017443164\n",
      "'ve 0.8210465427138288\n",
      "month -0.746771659958156\n",
      "sound 1.0064125311716876\n",
      "lot 0.7558694308596786\n",
      "you 0.9093131038945064\n",
      "n't -2.006786331514461\n",
      "easy 1.6943373947262834\n",
      "quality 1.4872475085090813\n",
      "company -0.5328623949847993\n",
      "card -0.5629654586610279\n",
      "item -1.0041309782833063\n",
      "wa -1.5409195846708101\n",
      "perfect 0.866012339884407\n",
      "fast 0.9122629148626061\n",
      "ha 0.6226826709096599\n",
      "price 2.7763570208169055\n",
      "value 0.5318504120756389\n",
      "money -1.1736535046717058\n",
      "memory 0.888509696761312\n",
      "buy -0.828682862115938\n",
      "bit 0.5768143336973105\n",
      "happy 0.6329521442048985\n",
      "pretty 0.7299569799249015\n",
      "doe -1.1274355631605615\n",
      "highly 0.9844785132773839\n",
      "recommend 0.6594237924527169\n",
      "fit 0.5926396754093266\n",
      "customer -0.648415189798466\n",
      "support -0.8446073815425573\n",
      "little 0.9928967227570831\n",
      "returned -0.723179986391548\n",
      "excellent 1.351580522356575\n",
      "love 1.2441045898107057\n",
      "home 0.523838727448427\n",
      "week -0.7036810402550863\n",
      "using 0.5180297285317852\n",
      "laptop 0.529580050010235\n",
      "video 0.5505816976201172\n",
      "poor -0.8433094798848008\n",
      "look 0.5299589853053958\n",
      "then -1.0999422047564094\n",
      "tried -0.8418093582222164\n",
      "try -0.6534885425377852\n",
      "space 0.5909253850120482\n",
      "comfortable 0.6552830390020251\n",
      "hour -0.5990822056787529\n",
      "expected 0.5724845041960269\n",
      "speaker 0.8632930895672967\n",
      "warranty -0.6342276281113306\n",
      "returning -0.5138958473026074\n",
      "paper 0.6263451022362376\n",
      "return -1.2195248231266358\n",
      "waste -1.0416195238017008\n",
      "refund -0.6093291401411666\n",
      "Most wrong positive review (prob = 0.3667875525581843, pred = 0.0):\n",
      "\n",
      "A device like this either works or it doesn't.  This one happens to work\n",
      "\n",
      "Most wrong negative review (prob = 0.6036585506065163, pred = 1.0):\n",
      "\n",
      "The Voice recorder meets all my expectations and more\n",
      "Easy to use, easy to transfer great results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# try it with different threshold values!\n",
    "threshold = 0.5\n",
    "# for word, index in iteritems(word_index_map):   # LazyP made mistake on this line !\n",
    "for word, index in word_index_map.items():        # This is my corrected version !\n",
    "    weight = model.coef_[0][index]\n",
    "    if weight > threshold or weight < -threshold:\n",
    "        print(word, weight)\n",
    "\n",
    "\n",
    "# check misclassified examples\n",
    "preds = model.predict(X)\n",
    "P = model.predict_proba(X)[:,1] # p(y = 1 | x)\n",
    "\n",
    "# since there are many, just print the \"most\" wrong samples\n",
    "minP_whenYis1 = 1\n",
    "maxP_whenYis0 = 0\n",
    "wrong_positive_review = None\n",
    "wrong_negative_review = None\n",
    "wrong_positive_prediction = None\n",
    "wrong_negative_prediction = None\n",
    "for i in range(N):\n",
    "    p = P[i]\n",
    "    y = Y[i]\n",
    "    if y == 1 and p < 0.5:\n",
    "        if p < minP_whenYis1:\n",
    "            wrong_positive_review = orig_reviews[i]\n",
    "            wrong_positive_prediction = preds[i]\n",
    "            minP_whenYis1 = p\n",
    "    elif y == 0 and p > 0.5:\n",
    "        if p > maxP_whenYis0:\n",
    "            wrong_negative_review = orig_reviews[i]\n",
    "            wrong_negative_prediction = preds[i]\n",
    "            maxP_whenYis0 = p\n",
    "\n",
    "print(\"Most wrong positive review (prob = %s, pred = %s):\" % (minP_whenYis1, wrong_positive_prediction))\n",
    "print(wrong_positive_review)\n",
    "print(\"Most wrong negative review (prob = %s, pred = %s):\" % (maxP_whenYis0, wrong_negative_prediction))\n",
    "print(wrong_negative_review)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## End of Code ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just playing --- printing out the first 3 positive reviews\n",
    "# basically trung out the .text attribute --- worked !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I purchased this unit due to frequent blackouts in my area and 2 power supplies going bad.  It will run my cable modem, router, PC, and LCD monitor for 5 minutes.  This is more than enough time to save work and shut down.   Equally important, I know that my electronics are receiving clean power.\n",
      "\n",
      "I feel that this investment is minor compared to the loss of valuable data or the failure of equipment due to a power spike or an irregular power supply.\n",
      "\n",
      "As always, Amazon had it to me in <2 business days\n",
      "**************************************************\n",
      "\n",
      "I ordered 3 APC Back-UPS ES 500s on the recommendation of an employee of mine who used to work at APC. I've had them for about a month now without any problems. They've functioned properly through a few unexpected power interruptions. I'll gladly order more if the need arises.\n",
      "\n",
      "Pros:\n",
      " - Large plug spacing, good for power adapters\n",
      " - Simple design\n",
      " - Long cord\n",
      "\n",
      "Cons:\n",
      " - No line conditioning (usually an expensive option\n",
      "**************************************************\n",
      "\n",
      "Wish the unit had a separate online/offline light.  When power to the unit is missing, the single red light turns off only when the warning sounds.  The warning sound is like a lot of sounds you hear in the house so it isn't always easy to tell what is happening\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "for i, review in enumerate(positive_reviews):\n",
    "    if i > 2:\n",
    "        break\n",
    "    print(positive_reviews[i].text + '*'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
